{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "image-2-image using diffusers",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thomose/diffusion-models/blob/master/image_2_image_using_diffusers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image2Image pipeline for Stabel Diffusion using dðŸ§¨ffusers \n",
        "\n",
        "This notebook shows how to create a custom `diffusers` pipeline for  text-guided image-to-image generation with Stable Diffusion model using  ðŸ¤— Hugging Face [ðŸ§¨ Diffusers library](https://github.com/huggingface/diffusers). \n",
        "\n",
        "For a general introduction to the Stabel Diffusion model please refer to this [colab](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion.ipynb).\n",
        "\n"
      ],
      "metadata": {
        "id": "846pN2uHGJLx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L52bmJXkAMQz"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qq -U diffusers transformers ftfy\n",
        "!pip install -qq \"ipywidgets>=7,<8\""
      ],
      "metadata": {
        "id": "n9AmMcAGASDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You need to accept the model license before downloading or using the weights. In this post we'll use model version `v1-4`, so you'll need to  visit [its card](https://huggingface.co/CompVis/stable-diffusion-v1-4), read the license and tick the checkbox if you agree. \n",
        "\n",
        "You have to be a registered user in ðŸ¤— Hugging Face Hub, and you'll also need to use an access token for the code to work. For more information on access tokens, please refer to [this section of the documentation](https://huggingface.co/docs/hub/security-tokens)."
      ],
      "metadata": {
        "id": "DQ11k9JVIXII"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "AOe3MmjoAoSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image2Image pipeline."
      ],
      "metadata": {
        "id": "Uclfq60nIQvm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import inspect\n",
        "import warnings\n",
        "from typing import List, Optional, Union\n",
        "\n",
        "import torch\n",
        "from torch import autocast\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from diffusers import (\n",
        "    AutoencoderKL,\n",
        "    DDIMScheduler,\n",
        "    DiffusionPipeline,\n",
        "    PNDMScheduler,\n",
        "    UNet2DConditionModel,\n",
        ")\n",
        "from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker\n",
        "from transformers import CLIPFeatureExtractor, CLIPTextModel, CLIPTokenizer"
      ],
      "metadata": {
        "id": "2YJrHBHlB54B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the pipeline for image2image"
      ],
      "metadata": {
        "id": "SNrHjKL6Kt4A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class StableDiffusionImg2ImgPipeline(DiffusionPipeline):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vae: AutoencoderKL,\n",
        "        text_encoder: CLIPTextModel,\n",
        "        tokenizer: CLIPTokenizer,\n",
        "        unet: UNet2DConditionModel,\n",
        "        scheduler: Union[DDIMScheduler, PNDMScheduler],\n",
        "        safety_checker: StableDiffusionSafetyChecker,\n",
        "        feature_extractor: CLIPFeatureExtractor,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        scheduler = scheduler.set_format(\"pt\")\n",
        "        self.register_modules(\n",
        "            vae=vae,\n",
        "            text_encoder=text_encoder,\n",
        "            tokenizer=tokenizer,\n",
        "            unet=unet,\n",
        "            scheduler=scheduler,\n",
        "            safety_checker=safety_checker,\n",
        "            feature_extractor=feature_extractor,\n",
        "        )\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def __call__(\n",
        "        self,\n",
        "        prompt: Union[str, List[str]],\n",
        "        init_image: torch.FloatTensor,\n",
        "        strength: float = 0.8,\n",
        "        num_inference_steps: Optional[int] = 50,\n",
        "        guidance_scale: Optional[float] = 7.5,\n",
        "        eta: Optional[float] = 0.0,\n",
        "        generator: Optional[torch.Generator] = None,\n",
        "        output_type: Optional[str] = \"pil\",\n",
        "    ):\n",
        "\n",
        "        if isinstance(prompt, str):\n",
        "            batch_size = 1\n",
        "        elif isinstance(prompt, list):\n",
        "            batch_size = len(prompt)\n",
        "        else:\n",
        "            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n",
        "\n",
        "        if strength < 0 or strength > 1:\n",
        "          raise ValueError(f'The value of strength should in [0.0, 1.0] but is {strength}')\n",
        "\n",
        "        # set timesteps\n",
        "        accepts_offset = \"offset\" in set(inspect.signature(self.scheduler.set_timesteps).parameters.keys())\n",
        "        extra_set_kwargs = {}\n",
        "        offset = 0\n",
        "        if accepts_offset:\n",
        "            offset = 1\n",
        "            extra_set_kwargs[\"offset\"] = 1\n",
        "\n",
        "        self.scheduler.set_timesteps(num_inference_steps, **extra_set_kwargs)\n",
        "\n",
        "        # encode the init image into latents and scale the latents\n",
        "        init_latents = self.vae.encode(init_image.to(self.device)).sample()\n",
        "        init_latents = 0.18215 * init_latents\n",
        "\n",
        "        # prepare init_latents noise to latents\n",
        "        init_latents = torch.cat([init_latents] * batch_size)\n",
        "        \n",
        "        # get the original timestep using init_timestep\n",
        "        init_timestep = int(num_inference_steps * strength) + offset\n",
        "        init_timestep = min(init_timestep, num_inference_steps)\n",
        "        timesteps = self.scheduler.timesteps[-init_timestep]\n",
        "        timesteps = torch.tensor([timesteps] * batch_size, dtype=torch.long, device=self.device)\n",
        "        \n",
        "        # add noise to latents using the timesteps\n",
        "        noise = torch.randn(init_latents.shape, generator=generator, device=self.device)\n",
        "        init_latents = self.scheduler.add_noise(init_latents, noise, timesteps)\n",
        "\n",
        "        # get prompt text embeddings\n",
        "        text_input = self.tokenizer(\n",
        "            prompt,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.tokenizer.model_max_length,\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        text_embeddings = self.text_encoder(text_input.input_ids.to(self.device))[0]\n",
        "\n",
        "        # here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)\n",
        "        # of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf . `guidance_scale = 1`\n",
        "        # corresponds to doing no classifier free guidance.\n",
        "        do_classifier_free_guidance = guidance_scale > 1.0\n",
        "        # get unconditional embeddings for classifier free guidance\n",
        "        if do_classifier_free_guidance:\n",
        "            max_length = text_input.input_ids.shape[-1]\n",
        "            uncond_input = self.tokenizer(\n",
        "                [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n",
        "            )\n",
        "            uncond_embeddings = self.text_encoder(uncond_input.input_ids.to(self.device))[0]\n",
        "\n",
        "            # For classifier free guidance, we need to do two forward passes.\n",
        "            # Here we concatenate the unconditional and text embeddings into a single batch\n",
        "            # to avoid doing two forward passes\n",
        "            text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
        "\n",
        "\n",
        "        # prepare extra kwargs for the scheduler step, since not all schedulers have the same signature\n",
        "        # eta (Î·) is only used with the DDIMScheduler, it will be ignored for other schedulers.\n",
        "        # eta corresponds to Î· in DDIM paper: https://arxiv.org/abs/2010.02502\n",
        "        # and should be between [0, 1]\n",
        "        accepts_eta = \"eta\" in set(inspect.signature(self.scheduler.step).parameters.keys())\n",
        "        extra_step_kwargs = {}\n",
        "        if accepts_eta:\n",
        "            extra_step_kwargs[\"eta\"] = eta\n",
        "\n",
        "        latents = init_latents\n",
        "        t_start = max(num_inference_steps - init_timestep + offset, 0)\n",
        "        for i, t in tqdm(enumerate(self.scheduler.timesteps[t_start:])):\n",
        "            # expand the latents if we are doing classifier free guidance\n",
        "            latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n",
        "\n",
        "            # predict the noise residual\n",
        "            noise_pred = self.unet(latent_model_input, t, encoder_hidden_states=text_embeddings)[\"sample\"]\n",
        "\n",
        "            # perform guidance\n",
        "            if do_classifier_free_guidance:\n",
        "                noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
        "                noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
        "\n",
        "            # compute the previous noisy sample x_t -> x_t-1\n",
        "            latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs)[\"prev_sample\"]\n",
        "\n",
        "        # scale and decode the image latents with vae\n",
        "        latents = 1 / 0.18215 * latents\n",
        "        image = self.vae.decode(latents)\n",
        "\n",
        "        image = (image / 2 + 0.5).clamp(0, 1)\n",
        "        image = image.cpu().permute(0, 2, 3, 1).numpy()\n",
        "\n",
        "        # run safety checker\n",
        "        safety_cheker_input = self.feature_extractor(self.numpy_to_pil(image), return_tensors=\"pt\").to(self.device)\n",
        "        image, has_nsfw_concept = self.safety_checker(images=image, clip_input=safety_cheker_input.pixel_values)\n",
        "\n",
        "        if output_type == \"pil\":\n",
        "            image = self.numpy_to_pil(image)\n",
        "\n",
        "        return {\"sample\": image, \"nsfw_content_detected\": has_nsfw_concept}"
      ],
      "metadata": {
        "id": "IQaAL0ROCAnC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the pipeline"
      ],
      "metadata": {
        "id": "YL179UtGKweb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\"\n",
        "model_path = \"CompVis/stable-diffusion-v1-4\"\n",
        "\n",
        "# Using DDIMScheduler as anexample,this also works with PNDMScheduler\n",
        "# uncomment this line if you want to use it.\n",
        "\n",
        "# scheduler = PNDMScheduler.from_config(model_path, subfolder=\"scheduler\", use_auth_token=True)\n",
        "\n",
        "scheduler = DDIMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", clip_sample=False, set_alpha_to_one=False)\n",
        "pipe = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
        "    model_path,\n",
        "    scheduler=scheduler,\n",
        "    revision=\"fp16\", \n",
        "    torch_dtype=torch.float16,\n",
        "    use_auth_token=True\n",
        ").to(device)"
      ],
      "metadata": {
        "id": "Fr2QIEzvCFH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download an initial image and preprocess it so we can pass it to the pipeline."
      ],
      "metadata": {
        "id": "tFBvRqfCKzVQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import PIL\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "def preprocess(image):\n",
        "    w, h = image.size\n",
        "    w, h = map(lambda x: x - x % 32, (w, h))  # resize to integer multiple of 32\n",
        "    image = image.resize((w, h), resample=PIL.Image.LANCZOS)\n",
        "    image = np.array(image).astype(np.float32) / 255.0\n",
        "    image = image[None].transpose(0, 3, 1, 2)\n",
        "    image = torch.from_numpy(image)\n",
        "    return 2.*image - 1."
      ],
      "metadata": {
        "id": "fP4i8LFkCqQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import requests\n",
        "from io import BytesIO\n",
        "\n",
        "#url = \"https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg\"\n",
        "\n",
        "#response = requests.get(url)\n",
        "import google.colab.files as files\n",
        "\n",
        "init_img = files.upload()"
      ],
      "metadata": {
        "id": "quipipaCC1AP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "init_img.keys()"
      ],
      "metadata": {
        "id": "7-l243LUAKUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cinit_img = Image.open(BytesIO(init_img['IMG_20201005_132847652.png'])).convert(\"RGB\")\n",
        "cinit_img = cinit_img.resize((512, 512))\n",
        "cinit_img"
      ],
      "metadata": {
        "id": "mSS3xLmg_0KN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocess the image."
      ],
      "metadata": {
        "id": "0s_heaqPK8OF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "init_image = preprocess(cinit_img)"
      ],
      "metadata": {
        "id": "Q9fI15SwDlZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the prompt and run the pipeline."
      ],
      "metadata": {
        "id": "XgtE5Qn9LE1m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Cowboy, Anime\""
      ],
      "metadata": {
        "id": "4V11Z0l-ZUWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, `strength` is a value between 0.0 and 1.0, that controls the amount of noise that is added to the input image. Values that approach 1.0 allow for lots of variations but will also produce images that are not semantically consistent with the input."
      ],
      "metadata": {
        "id": "8wOmwKnkDfpd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generator = torch.Generator(device=device).manual_seed(1024)\n",
        "with autocast(\"cuda\"):\n",
        "    images = pipe(prompt=prompt, init_image=init_image, strength=0.4, guidance_scale=7.5, generator=generator)[\"sample\"]"
      ],
      "metadata": {
        "id": "V24njWQBC8eC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images[0]"
      ],
      "metadata": {
        "id": "uMfaTT24DWk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "images[0].save(\"/content/gdrive/My Drive/cowboy_thomas.png\")"
      ],
      "metadata": {
        "id": "8MKjlOSsCHId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with autocast(\"cuda\"):\n",
        "    images = pipe(prompt=prompt, init_image=init_image, strength=0.5, guidance_scale=7.5, generator=generator)[\"sample\"]"
      ],
      "metadata": {
        "id": "Rfn_M2f9Dsl1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images[0]"
      ],
      "metadata": {
        "id": "JlUXXh7MEFPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, when using a lower value for `strength`, the generated image is more closer to the original `init_image`"
      ],
      "metadata": {
        "id": "NlQlTZDIEQKi"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9OIPklNaEeBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using PNDMScheduler (PLMS)."
      ],
      "metadata": {
        "id": "kpBvfS4oVv9K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scheduler = PNDMScheduler.from_config(model_path, subfolder=\"scheduler\", use_auth_token=True)\n",
        "pipe.scheduler = scheduler"
      ],
      "metadata": {
        "id": "0O0Iy1zK0q-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator = torch.Generator(device=device).manual_seed(1024)\n",
        "with autocast(\"cuda\"):\n",
        "    images = pipe(prompt=prompt, init_image=init_image, strength=0.75, guidance_scale=7.5, generator=generator)[\"sample\"]"
      ],
      "metadata": {
        "id": "q7ixTMhfD6Ux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images[0]"
      ],
      "metadata": {
        "id": "Tm_SR0gT0s2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "45csVGg7vYO8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}